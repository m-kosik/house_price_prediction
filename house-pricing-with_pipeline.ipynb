{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ad259a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-19T12:57:00.204107Z",
     "iopub.status.busy": "2021-11-19T12:57:00.203404Z",
     "iopub.status.idle": "2021-11-19T12:57:00.268708Z",
     "shell.execute_reply": "2021-11-19T12:57:00.26804Z",
     "shell.execute_reply.started": "2021-11-19T12:57:00.204062Z"
    },
    "papermill": {
     "duration": 0.009083,
     "end_time": "2021-11-19T19:29:03.723020",
     "exception": false,
     "start_time": "2021-11-19T19:29:03.713937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is based entirely on the knowledge which I gained from doing the Machine Learning courses on Kaggle and reading sklearn/pandas documentation.  \n",
    "\n",
    "I tried here to create an automated pipeline for feature preparation, which does:\n",
    "- remove numerical columns with a large amount of missing data,\n",
    "- impute missing data in categorical columns and in the numerical columns which have most entries and a small part of missing data,\n",
    "- removes features have small mutual information with the target,\n",
    "- performs One Hot Encoding on the categorical data.\n",
    "\n",
    "Next, the preprocessed data is fed into a simple XGBRegressor model, which can be run with early stopping rounds, when performed on the train/validations sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a654e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T19:29:03.748333Z",
     "iopub.status.busy": "2021-11-19T19:29:03.747335Z",
     "iopub.status.idle": "2021-11-19T19:29:05.186883Z",
     "shell.execute_reply": "2021-11-19T19:29:05.187499Z",
     "shell.execute_reply.started": "2021-11-19T19:25:27.462937Z"
    },
    "papermill": {
     "duration": 1.456809,
     "end_time": "2021-11-19T19:29:05.187831",
     "exception": false,
     "start_time": "2021-11-19T19:29:03.731022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('../input/home-data-for-ml-course/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../input/home-data-for-ml-course/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41547e6",
   "metadata": {
    "papermill": {
     "duration": 0.007344,
     "end_time": "2021-11-19T19:29:05.203009",
     "exception": false,
     "start_time": "2021-11-19T19:29:05.195665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, I define the function which will preprocess data. \n",
    "It takes as arguments: \n",
    "- a train set `X` \n",
    "- a test/validation set `X_test` \n",
    "- the target output as `y`.  \n",
    "\n",
    "It works on copies of the `X` and `X_test` data, and returns these two datasets after completing all preprocessing steps, which include:\n",
    "- removing numerical columns which have more than `missing_threshold` missing entries,\n",
    "- imputing missing data in the rest of the numerical columns with missing data using the median value or 0,\n",
    "- imputing missing data in categorical columns with a new value (often the NaN in categorical data has the meaning of a new category, e.g. \"the absence of a garage\") \n",
    "- removing features have mutual information with the target smaller than `mutual_inf_threshold`,\n",
    "- dropping categorical data that have cardinality larger than `low_cardinality_threshold`,\n",
    "- performing One Hot Encoding on the low-cardinality categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79502629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T19:29:05.230618Z",
     "iopub.status.busy": "2021-11-19T19:29:05.223517Z",
     "iopub.status.idle": "2021-11-19T19:29:05.254078Z",
     "shell.execute_reply": "2021-11-19T19:29:05.253484Z",
     "shell.execute_reply.started": "2021-11-19T19:25:27.535406Z"
    },
    "papermill": {
     "duration": 0.043687,
     "end_time": "2021-11-19T19:29:05.254232",
     "exception": false,
     "start_time": "2021-11-19T19:29:05.210545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "def preprocessing_pipeline(X, y, X_test, \n",
    "                           missing_threshold=100, \n",
    "                           mutual_inf_threshold=0.05, \n",
    "                           low_cardinality_threshold=15,\n",
    "                           verbose = False):\n",
    "    \n",
    "    data = X.copy()\n",
    "    data_test = X_test.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print('**************************')\n",
    "        print('\\nNumber of features at the beginning:')\n",
    "        print(len(data.columns))\n",
    "        print(len(data_test.columns))\n",
    "    \n",
    "    #--- Impute missing values\n",
    "    cols_with_missing = set([col for col in data.columns if data[col].isnull().any()])\n",
    "    cols_with_missing.update([col for col in data_test.columns if data_test[col].isnull().any()])\n",
    "    if verbose:\n",
    "        print('Columns with missing values: ')\n",
    "        print(cols_with_missing)\n",
    "    \n",
    "    cols_to_drop = [col for col in cols_with_missing if data[col].isnull().sum()>missing_threshold] \n",
    "    cols_with_missing_num = [col for col in cols_with_missing if data[col].isnull().sum()<missing_threshold and data[col].dtype != \"object\"] \n",
    "    cols_with_missing_cat = [col for col in cols_with_missing if data[col].isnull().sum()<missing_threshold and data[col].dtype == \"object\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n Columns which will be dropped due to a lot of missing values: ')\n",
    "        print(cols_to_drop)\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    data_test = data_test.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nNumber of features after dropping missing values:')\n",
    "        print(len(data.columns))\n",
    "        print(len(data_test.columns))\n",
    "    \n",
    "    for column in cols_with_missing_num:\n",
    "        if column == 'LotFrontage':\n",
    "            data[column] = data[column].fillna(data[column].median())\n",
    "            data_test[column] = data_test[column].fillna(data_test[column].median())\n",
    "        else:\n",
    "            data[column] = data[column].fillna(0)\n",
    "            data_test[column] = data_test[column].fillna(0)\n",
    "    \n",
    "    for column in cols_with_missing_cat:\n",
    "        data[column] = data[column].fillna('N')\n",
    "        data_test[column] = data_test[column].fillna('N')\n",
    "    \n",
    "    imputed = data.copy()\n",
    "    imputed_test = data_test.copy()\n",
    "\n",
    "    imputed.columns = data.columns\n",
    "    imputed_test.columns = data_test.columns\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nNumber of features after imputation:')\n",
    "        print(len(imputed.columns))\n",
    "        print(len(imputed_test.columns))\n",
    "    \n",
    "    #--- Throw away numerical features which have very small mutual information with target\n",
    "    num_cols = [col for col in imputed.columns if imputed[col].dtypes != 'object']\n",
    "    \n",
    "    num = (data.dtypes != 'object')\n",
    "    num_cols = list(num[num].index)\n",
    "    \n",
    "    mi_scores = make_mi_scores(imputed[num_cols], y, 'auto')\n",
    "    if verbose:\n",
    "        print('\\nNumerical features with highest mutual information with target: ')\n",
    "        print(mi_scores)\n",
    "    \n",
    "    unimportant_columns = [index for index,score in mi_scores.iteritems() if score < mutual_inf_threshold]\n",
    "    if verbose:\n",
    "        print('\\nColumns which will be dropped: ')\n",
    "        print(unimportant_columns)\n",
    "    \n",
    "    high_mi = imputed.drop(unimportant_columns, axis=1)\n",
    "    high_mi_test = imputed_test.drop(unimportant_columns, axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nNumber of features after dropping low MI features:')\n",
    "        print(len(high_mi.columns))\n",
    "        print(len(high_mi_test.columns))\n",
    "\n",
    "    #--- Encode categorical features\n",
    "    \n",
    "    cat = (high_mi.dtypes == 'object')\n",
    "    categorical_cols = list(cat[cat].index)\n",
    "    low_cardinality_cols = [col for col in categorical_cols if high_mi[col].nunique() < low_cardinality_threshold]\n",
    "    high_cardinality_cols = list(set(categorical_cols)-set(low_cardinality_cols))\n",
    "    \n",
    "    lc_X_train = high_mi.drop(high_cardinality_cols, axis=1)\n",
    "    lc_X_valid = high_mi_test.drop(high_cardinality_cols, axis=1)\n",
    "    \n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(lc_X_train[low_cardinality_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(lc_X_valid[low_cardinality_cols]))\n",
    "    \n",
    "    OH_cols_train.index = high_mi.index\n",
    "    OH_cols_valid.index = high_mi_test.index\n",
    "    num_X_train = high_mi.drop(categorical_cols, axis=1)\n",
    "    num_X_valid = high_mi_test.drop(categorical_cols, axis=1)\n",
    "    OH_X = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_test = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nNumber of features after OHE:')\n",
    "        print(len(OH_X.columns)) \n",
    "        print(len(OH_X_test.columns))\n",
    "    \n",
    "    return OH_X, OH_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac86e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T18:58:54.38704Z",
     "iopub.status.busy": "2021-11-19T18:58:54.386219Z",
     "iopub.status.idle": "2021-11-19T18:58:54.414167Z",
     "shell.execute_reply": "2021-11-19T18:58:54.413283Z",
     "shell.execute_reply.started": "2021-11-19T18:58:54.38693Z"
    },
    "papermill": {
     "duration": 0.007363,
     "end_time": "2021-11-19T19:29:05.269271",
     "exception": false,
     "start_time": "2021-11-19T19:29:05.261908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, during the first call the preprocessing pipeline is applied to the train and validations sets. These two sets will be used for validating the model and searching best parameters.    \n",
    "\n",
    "Below, the preprocessing pipeline is applied to the full training set and the test set. These two sets will be used for the final training of the model with best parameters and generating the predictions for the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a54cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T19:29:05.294994Z",
     "iopub.status.busy": "2021-11-19T19:29:05.293906Z",
     "iopub.status.idle": "2021-11-19T19:36:04.416949Z",
     "shell.execute_reply": "2021-11-19T19:36:04.417535Z",
     "shell.execute_reply.started": "2021-11-19T19:25:27.567975Z"
    },
    "papermill": {
     "duration": 419.140949,
     "end_time": "2021-11-19T19:36:04.417965",
     "exception": false,
     "start_time": "2021-11-19T19:29:05.277016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: 500, 0, 5\n",
      "**********************************************\n",
      "500\n",
      "0\n",
      "5\n",
      "17372.77334385702\n",
      "running: 500, 0, 10\n",
      "**********************************************\n",
      "500\n",
      "0\n",
      "10\n",
      "16748.630912885274\n",
      "running: 500, 0, 20\n",
      "**********************************************\n",
      "500\n",
      "0\n",
      "20\n",
      "16451.913848458906\n",
      "running: 500, 0.02, 5\n",
      "**********************************************\n",
      "500\n",
      "0.02\n",
      "5\n",
      "17060.511584974316\n",
      "running: 500, 0.02, 10\n",
      "**********************************************\n",
      "500\n",
      "0.02\n",
      "10\n",
      "16606.881969713184\n",
      "running: 500, 0.02, 20\n",
      "**********************************************\n",
      "500\n",
      "0.02\n",
      "20\n",
      "16392.254414597603\n",
      "running: 500, 0.05, 5\n",
      "**********************************************\n",
      "500\n",
      "0.05\n",
      "5\n",
      "17045.279350385274\n",
      "running: 500, 0.05, 10\n",
      "**********************************************\n",
      "500\n",
      "0.05\n",
      "10\n",
      "16729.266748715752\n",
      "running: 500, 0.05, 20\n",
      "**********************************************\n",
      "500\n",
      "0.05\n",
      "20\n",
      "16428.584305436645\n",
      "running: 500, 0.1, 5\n",
      "**********************************************\n",
      "500\n",
      "0.1\n",
      "5\n",
      "17419.006728916953\n",
      "running: 500, 0.1, 10\n",
      "**********************************************\n",
      "500\n",
      "0.1\n",
      "10\n",
      "16578.55867401541\n",
      "running: 500, 0.1, 20\n",
      "**********************************************\n",
      "500\n",
      "0.1\n",
      "20\n",
      "16563.600786601026\n",
      "running: 1000, 0, 5\n",
      "**********************************************\n",
      "1000\n",
      "0\n",
      "5\n",
      "17372.77334385702\n",
      "running: 1000, 0, 10\n",
      "**********************************************\n",
      "1000\n",
      "0\n",
      "10\n",
      "16693.03487532106\n",
      "running: 1000, 0, 20\n",
      "**********************************************\n",
      "1000\n",
      "0\n",
      "20\n",
      "16588.393327268837\n",
      "running: 1000, 0.02, 5\n",
      "**********************************************\n",
      "1000\n",
      "0.02\n",
      "5\n",
      "17053.149895654966\n",
      "running: 1000, 0.02, 10\n",
      "**********************************************\n",
      "1000\n",
      "0.02\n",
      "10\n",
      "16510.474208047945\n",
      "running: 1000, 0.02, 20\n",
      "**********************************************\n",
      "1000\n",
      "0.02\n",
      "20\n",
      "16247.342184824487\n",
      "running: 1000, 0.05, 5\n",
      "**********************************************\n",
      "1000\n",
      "0.05\n",
      "5\n",
      "17045.279350385274\n",
      "running: 1000, 0.05, 10\n",
      "**********************************************\n",
      "1000\n",
      "0.05\n",
      "10\n",
      "16690.235030500855\n",
      "running: 1000, 0.05, 20\n",
      "**********************************************\n",
      "1000\n",
      "0.05\n",
      "20\n",
      "16487.602124357876\n",
      "running: 1000, 0.1, 5\n",
      "**********************************************\n",
      "1000\n",
      "0.1\n",
      "5\n",
      "16958.200516374145\n",
      "running: 1000, 0.1, 10\n",
      "**********************************************\n",
      "1000\n",
      "0.1\n",
      "10\n",
      "17335.622190710616\n",
      "running: 1000, 0.1, 20\n",
      "**********************************************\n",
      "1000\n",
      "0.1\n",
      "20\n",
      "17132.17016267123\n",
      "running: 1500, 0, 5\n",
      "**********************************************\n",
      "1500\n",
      "0\n",
      "5\n",
      "17353.762039811645\n",
      "running: 1500, 0, 10\n",
      "**********************************************\n",
      "1500\n",
      "0\n",
      "10\n",
      "16725.10132170377\n",
      "running: 1500, 0, 20\n",
      "**********************************************\n",
      "1500\n",
      "0\n",
      "20\n",
      "16556.136370933218\n",
      "running: 1500, 0.02, 5\n",
      "**********************************************\n",
      "1500\n",
      "0.02\n",
      "5\n",
      "17056.238134096748\n",
      "running: 1500, 0.02, 10\n",
      "**********************************************\n",
      "1500\n",
      "0.02\n",
      "10\n",
      "16521.540948737158\n",
      "running: 1500, 0.02, 20\n",
      "**********************************************\n",
      "1500\n",
      "0.02\n",
      "20\n",
      "16416.771872324487\n",
      "running: 1500, 0.05, 5\n",
      "**********************************************\n",
      "1500\n",
      "0.05\n",
      "5\n",
      "16998.641400898974\n",
      "running: 1500, 0.05, 10\n",
      "**********************************************\n",
      "1500\n",
      "0.05\n",
      "10\n",
      "16701.115903253423\n",
      "running: 1500, 0.05, 20\n",
      "**********************************************\n",
      "1500\n",
      "0.05\n",
      "20\n",
      "16350.919105843323\n",
      "running: 1500, 0.1, 5\n",
      "**********************************************\n",
      "1500\n",
      "0.1\n",
      "5\n",
      "17161.38843107877\n",
      "running: 1500, 0.1, 10\n",
      "**********************************************\n",
      "1500\n",
      "0.1\n",
      "10\n",
      "16759.450984589042\n",
      "running: 1500, 0.1, 20\n",
      "**********************************************\n",
      "1500\n",
      "0.1\n",
      "20\n",
      "16532.669774721748\n",
      "16247.342184824487\n",
      "(1000, 0.02, 20)\n"
     ]
    }
   ],
   "source": [
    "smallest_mae = 30_000\n",
    "\n",
    "for missing_threshold in [500,1000,1500]:\n",
    "    for mutual_inf_threshold in [0,0.02,0.05,0.1]:\n",
    "        for low_cardinality_threshold in [5,10,20]:\n",
    "            print(f'running: {missing_threshold}, {mutual_inf_threshold}, {low_cardinality_threshold}')\n",
    "            \n",
    "            processed_X_train, processed_X_valid = preprocessing_pipeline(X_train, y_train, X_valid, \n",
    "                                                                          missing_threshold=missing_threshold,      \n",
    "                                                                          mutual_inf_threshold=mutual_inf_threshold, \n",
    "                                                                          low_cardinality_threshold=low_cardinality_threshold)\n",
    "            \n",
    "            model = XGBRegressor(n_estimators=5_000, learning_rate = 0.01)\n",
    "            model.fit(processed_X_train, y_train, early_stopping_rounds=10, \n",
    "                      eval_set=[(processed_X_valid, y_valid)],verbose=False)\n",
    "            preds_test = model.predict(processed_X_valid)\n",
    "            \n",
    "            print('**********************************************')\n",
    "            print(missing_threshold)\n",
    "            print(mutual_inf_threshold)\n",
    "            print(low_cardinality_threshold)\n",
    "            mae = mean_absolute_error(y_valid, preds_test)\n",
    "            print(mae)\n",
    "            if mae < smallest_mae:\n",
    "                smallest_mae = mae\n",
    "                best_params = missing_threshold, mutual_inf_threshold, low_cardinality_threshold\n",
    "                \n",
    "print(smallest_mae)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae55f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T19:27:54.652735Z",
     "iopub.status.busy": "2021-11-19T19:27:54.651732Z",
     "iopub.status.idle": "2021-11-19T19:27:54.659187Z",
     "shell.execute_reply": "2021-11-19T19:27:54.658127Z",
     "shell.execute_reply.started": "2021-11-19T19:27:54.652669Z"
    },
    "papermill": {
     "duration": 0.019434,
     "end_time": "2021-11-19T19:36:04.456257",
     "exception": false,
     "start_time": "2021-11-19T19:36:04.436823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, the best parameters will be used to the generate predictions on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39c9426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T19:36:04.503000Z",
     "iopub.status.busy": "2021-11-19T19:36:04.502325Z",
     "iopub.status.idle": "2021-11-19T19:37:05.921137Z",
     "shell.execute_reply": "2021-11-19T19:37:05.921682Z",
     "shell.execute_reply.started": "2021-11-19T19:26:01.144340Z"
    },
    "papermill": {
     "duration": 61.446472,
     "end_time": "2021-11-19T19:37:05.921860",
     "exception": false,
     "start_time": "2021-11-19T19:36:04.475388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 252\n",
      "output saved\n"
     ]
    }
   ],
   "source": [
    "processed_X, processed_X_test = preprocessing_pipeline(X, y, X_test, \n",
    "                                                       missing_threshold=best_params[0],      \n",
    "                                                       mutual_inf_threshold=best_params[1],\n",
    "                                                       low_cardinality_threshold=best_params[2])\n",
    "print(len(processed_X.columns), len(processed_X_test.columns))\n",
    "\n",
    "model = XGBRegressor(n_estimators = 5_000, learning_rate = 0.01)\n",
    "model.fit(processed_X, y)\n",
    "preds_test = model.predict(processed_X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print('output saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84192602",
   "metadata": {
    "papermill": {
     "duration": 0.019362,
     "end_time": "2021-11-19T19:37:05.960896",
     "exception": false,
     "start_time": "2021-11-19T19:37:05.941534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The last thing which remains would be optimize the XGBRegressor parameters, e.g. using GridSearchCV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 492.421012,
   "end_time": "2021-11-19T19:37:06.692509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-19T19:28:54.271497",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
